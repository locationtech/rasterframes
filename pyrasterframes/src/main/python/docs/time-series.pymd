# Time Series

## Analysis Plan

```python setup, echo=False
from IPython.display import display
import pyrasterframes
from pyrasterframes.rasterfunctions import *
import pyrasterframes.rf_ipython
# This job is more memory bound, so reduce the concurrent tasks.
spark = pyrasterframes.get_spark_session("local[4]")
```

In this example, we will show how the flexibility of the DataFrame concept for raster data allows a simple and intuitive way to extract a time series from Earth observation data. We will start with our @ref:[built-in MODIS data catalog](raster-catalogs.md#using-built-in-experimental-catalogs).

```python catalog
cat = spark.read.format('aws-pds-modis-catalog').load().repartition(200)
cat.printSchema()
```

We will summarize the change in NDVI over 2018 in the Cuyahoga Valley National Park in Ohio, USA. First, we will retrieve open vector data delineating the park boundary from the US National Park Service's LandsNet.

```python get_park_boundary
import requests
import geopandas
nps_data_query_url = 'https://services1.arcgis.com/fBc8EJBxQRMcHlei/arcgis/rest/services/NPS_Park_Boundaries/FeatureServer/0/query?where=1%3D1&outFields=*&geometry=-82.451%2C41.075%2C-80.682%2C41.436&geometryType=esriGeometryEnvelope&inSR=4326&spatialRel=esriSpatialRelIntersects&outSR=4326&f=json'
r = requests.get(nps_data_query_url)
with open('/tmp/parks.geojson', 'wb') as f:
    for chunk in r.iter_content(chunk_size=128):
        f.write(chunk)

park_df = geopandas.read_file('/tmp/parks.geojson')
park_geo = park_df[park_df.UNIT_CODE=='CUVA'].geometry[0]

park_geo.wkt[:100]
```


The entire park boundary is contained in MODIS granule h11 v4. We will simply filter on this granule, rather than using a @ref:[spatial relation](vector-data.md#geomesa-functions-and-spatial-relations). The time period selected should show the change in plant vigor as leaves emerge over the spring and into early summer.

```python query_catalog
from pyspark.sql.functions import lit
park_cat = cat.filter(
                    (cat.granule_id == 'h11v04') &
                    (cat.acquisition_date > lit('2018-02-19')) &
                    (cat.acquisition_date < lit('2018-07-01'))
            )
park_cat.printSchema()
```

## Catalog Read

Now we have a catalog with several months of MODIS data for a single granule. However, the granule is larger than our park boundary. We will combine the park geometry with the catalog, and read only the bands of interest to compute NDVI, which we discussed in a @ref:[previous section](local-algebra.md#computing-ndvi).

```python read_catalog
raster_cols = ['B01', 'B02',] # red and near-infrared respectively
park_rf = spark.read.raster(
    catalog=park_cat.select(['acquisition_date', 'granule_id'] + raster_cols),
    catalog_col_names=raster_cols
    ) \
    .withColumn('park', st_geomFromWKT(lit(park_geo.wkt)))
park_rf.printSchema()
park_rf.persist()                    
```

## Vector and Raster Data Interaction

Now we have the vector representation of the park boundary alongside the _tiles_ of red and near infrared bands. Next, we need to create a _tile_ representation of the park to allow us to limit the time series analysis to pixels within the park. This is similar to the masking operation demonstrated in @ref:[NoData handling](nodata-handling.md#masking).

We do this using two transformations. The first one will reproject the park boundary from coordinates to the MODIS sinusoidal projection. The second one will create a new _tile_ aligned with the imagery containing a value of 1 where the pixels are contained within the park and NoData elsewhere. 

```python burn_in
cr_1 = park_rf.withColumn('park_native', st_reproject('park', lit('EPSG:4326'), rf_crs('B01')))

cr_2 = cr_1 \
    .withColumn('dims', rf_dimensions('B01')) \
    .withColumn('park_tile', rf_rasterize('park_native', rf_geometry('B01'), lit(1), 'dims.cols', 'dims.rows')) \
    .where(rf_tile_sum('park_tile') > 0)
cr_2.printSchema()
cr_2.persist()
```

## Create Time Series

Next, we will compute NDVI as the normalized difference of near infrared (band 2) and red (band 1). The _tiles_ are masked by the `park_tile`. We will then aggregate across the remaining values to arrive at an average NDVI for each week of the year. Note that the computation is creating a weighted average, which is weighted by the number of valid observations per week.

```python ndvi_time_series
from pyspark.sql.functions import col, year, weekofyear, month
from pyspark.sql.functions import sum as sql_sum

rf_ndvi = cr_2.withColumn('ndvi', rf_normalized_difference('B02', 'B01')) \
              .withColumn('ndvi_masked', rf_mask('ndvi', 'park_tile'))

time_series = rf_ndvi \
        .withColumn('ndvi_wt', rf_tile_sum('ndvi_masked')) \
        .withColumn('wt', rf_data_cells('ndvi_masked')) \
        .groupby(year('acquisition_date').alias('year'), weekofyear('acquisition_date').alias('week')) \
        .agg(sql_sum('ndvi_wt').alias('ndvi_wt_wk'), sql_sum('wt').alias('wt_wk')) \
        .withColumn('ndvi', col('ndvi_wt_wk') / col('wt_wk'))
time_series.printSchema()
time_series.persist()
```

Finally, we will take a look at the NDVI over time.

```python time_series_display
import matplotlib.pyplot as plt

time_series_pdf = time_series.toPandas()
time_series_pdf = time_series_pdf.sort_values('week')
plt.plot(time_series_pdf['week'], time_series_pdf['ndvi'], 'go-')
plt.xlabel('Week of year, 2018')
plt.ylabel('NDVI')
plt.title('Cuyahoga Valley NP Green-up')
```

We can see two fairly clear elbows in the curve at week 17 and week 21, indicating the start and end of the green up period. Estimation of such parameters is one technique [phenology](https://en.wikipedia.org/wiki/Phenology) researchers use to monitor changes in climate and environment.
