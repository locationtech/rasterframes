# Getting Started

There are @ref:[several ways](getting-started.md#other-options) to use RasterFrames. Let's start with the simplest. Python 3.6 or greater is recommended.

```bash
$ python3 -m pip install pyrasterframes
```

Then in a python interpreter of your choice, you can get a [`pyspark` `SparkSession`](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession) using the [`local[*]` master](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls).

```python
import pyrasterframes
spark = pyrasterframes.get_spark_session()
```

Then you can read a raster and work with it in a Spark DataFrame.

```python
from pyrasterframes.rasterfunctions import rf_local_add
from pyspark.sql.functions import lit

# Read a MODIS surface reflectance granule
df = spark.read.rastersource('https://modis-pds.s3.amazonaws.com/MCD43A4.006/11/08/2019059/MCD43A4.A2019059.h11v08.006.2019072203257_B02.TIF')

# Add 3 element-wise, show some rows of the dataframe
df.select(rf_local_add(df.proj_raster, lit(3))).show(5, False)

```

This example is extended in the [getting started Jupyter notebook](https://nbviewer.jupyter.org/github/locationtech/rasterframes/blob/develop/rf-notebook/src/main/notebooks/Getting%20Started.ipynb).

To understand more about how and why RasterFrames represents Earth observation in DataFrames, read the project @ref:[description](description.md). For more hands-on examples, see the chapters about @ref:[reading](raster-io.md) and @ref:[processing](raster-processing.md) with RasterFrames.

## Raster Functions

To import RasterFrames functions into the environment, import from `pyrasterframes.rasterfunctions`.

```python
from pyrasterframes.rasterfunctions import *
```

Functions starting with `rf_`, which are for raster, and `st_`, which are for vector geometry,
become available for use with DataFrames. See the complete @ref:[function reference](reference.md).

```python
[fn for fn in dir() if fn.startswith('rf_') or fn.startswith('st_')]
```

## Other Options

You can also use RasterFrames in the following environments:

1. Jupyter Notebook
1. `pyspark` shell

### Jupyter Notebook

**TODO** User facing quick instructions e.g. how to pull and run docker hub hosted container

See [RasterFrames Notebook README](https://github.com/locationtech/rasterframes/blob/develop/rf-notebook/README.md) for instructions on running a Jupyter notebook server within a Docker container that has a fully set up environment.

### `pyspark` shell or app

To initialize RasterFrames in a `pyspark` shell, prepare to call pyspark with the appropriate `--master` and other `--conf` arguments for your cluster manager and environment. To these you will add the PyRasterFrames assembly JAR and the python source zip.

**TODO** how to build or download those artifacts.

```bash
   pyspark \
    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
    --conf spark.kryo.registrator=org.locationtech.rasterframes.util.RFKryoRegistrator \
    --conf spark.kryoserializer.buffer.max=500m \
    --jars pyrasterframes/target/scala-2.11/pyrasterframes-assembly-${VERSION}.jar \
    --py-files pyrasterframes/target/scala-2.11/pyrasterframes-python-${VERSION}.zip
```

Then in the pyspark shell, import the module and call `withRasterFrames` on the SparkSession.

```python, evaluate=False
import pyrasterframes
spark = spark.withRasterFrames()
df = spark.read.rastersource('https://landsat-pds.s3.amazonaws.com/c1/L8/158/072/LC08_L1TP_158072_20180515_20180604_01_T1/LC08_L1TP_158072_20180515_20180604_01_T1_B5.TIF')
```

Now you have the configured SparkSession with RasterFrames enabled.


```python, echo=False 
spark.stop()
```


