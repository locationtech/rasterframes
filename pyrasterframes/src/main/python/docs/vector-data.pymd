# Vector Data

RasterFrames provides a variety of ways to work with spatial vector data (points, lines, and polygons) alongside raster data. There is a convenience DataSource for the GeoJSON format, as well as the ability to convert from [GeoPandas][GeoPandas] to Spark. Representation of vector geometries in PySpark is through [Shapely][Shapely], providing a great deal of interoperability. RasterFrames also provides access to Spark functions for working with geometries.

## GeoJSON DataSource

```python, setup, echo=False
import pyrasterframes
import pyrasterframes.rf_ipython
from pyrasterframes.utils import create_rf_spark_session
spark = create_rf_spark_session()
```

```python, read_geojson
from pyspark import SparkFiles
spark.sparkContext.addFile('https://raw.githubusercontent.com/datasets/geo-admin1-us/master/data/admin1-us.geojson')

df = spark.read.geojson(SparkFiles.get('admin1-us.geojson'))
df.printSchema()
```

The properties of each discrete geometry are available as columns of the DataFrame, along with the geometry itself.

## GeoPandas and RasterFrames

You can also convert a [GeoPandas][GeoPandas] GeoDataFrame to a Spark DataFrame, preserving the geometry column. This means that any vector format that can be read with [OGR][OGR] can be converted to a Spark DataFrame. In the example below, we expect the same schema as the DataFrame defined above by the GeoJSON reader. Note that in a GeoPandas DataFrame there can be heterogeneous geometry types in the column, which may fail Spark's schema inference.

```python, read_and_normalize
import geopandas
from shapely.geometry import MultiPolygon

def poly_or_mp_to_mp(g):
    """ Normalize polygons or multipolygons to all be multipolygons. """
    if isinstance(g, MultiPolygon):
        return g
    else:
        return MultiPolygon([g])

gdf = geopandas.read_file('https://raw.githubusercontent.com/datasets/geo-admin1-us/master/data/admin1-us.geojson')
gdf.geometry = gdf.geometry.apply(poly_or_mp_to_mp)
df2 = spark.createDataFrame(gdf)
df2.printSchema()
```

## Shapely Geometry Support

The `geometry` column will have a Spark user-defined type that is compatible with [Shapely][Shapely] when working with Python via PySpark. This means that when the data is collected to the driver, it will be a Shapely geometry object.

```python, show_geom
the_first = df.first()
print(type(the_first['geometry']))
```

Since it is a geometry we can do things like this:

```python, show_wkt
the_first['geometry'].wkt
```

You can also write user-defined functions that take geometries as input, output, or both, via user defined types in the [geomesa_pyspark.types](https://github.com/locationtech/rasterframes/blob/develop/pyrasterframes/src/main/python/geomesa_pyspark/types.py) module. Here is a simple example of a user-defined function that uses both a geometry input and output to compute the centroid of a geometry.

```python, add_centroid
from pyspark.sql.functions import udf
from geomesa_pyspark.types import PointUDT

@udf(PointUDT())
def get_centroid(g):
    return g.centroid

df = df.withColumn('naive_centroid', get_centroid(df.geometry))
df.printSchema()
```

We can take a look at a sample of the data. Notice the geometry columns print as well known text (wkt).

```python, show_centroid
display(df.limit(4))
```


## GeoMesa Functions and Spatial Relations

As documented in the @ref:[function reference](reference.md), various user-defined functions implemented by GeoMesa are also available for use. The example below uses a GeoMesa user-defined function to compute the centroid of a geometry. It is logically equivalent to the example above, but more efficient.


```python, native_centroid
from pyrasterframes.rasterfunctions import st_centroid
df = df.withColumn('centroid', st_centroid(df.geometry))
centroids = df.select('name', 'geometry', 'naive_centroid', 'centroid')
display(centroids.limit(4))
```

The RasterFrames vector functions and GeoMesa functions also provide a variety of spatial relations that are useful in combination with the geometric properties of projected rasters. In this example, we use the @ref:[built-in Landsat catalog](raster-catalogs.md#using-built-in-experimental-catalogs) which provides an extent. We will convert the extent to a polygon and filter to those within approximately 500 km of a selected point.

```python, evaluate=True
from pyrasterframes.rasterfunctions import st_geometry, st_bufferPoint, st_intersects, st_point
from pyspark.sql.functions import lit
l8 = spark.read.format('aws-pds-l8-catalog').load()

l8 = l8.withColumn('geom', st_geometry(l8.bounds_wgs84))
l8 = l8.withColumn('paducah', st_point(lit(-88.6275), lit(37.072222)))

l8_filtered = l8.filter(st_intersects(l8.geom, st_bufferPoint(l8.paducah, lit(500000.0))))
```

```python, evaluate=False, echo=False
# suppressed due to run time.
l8_filtered.count()
```


[GeoPandas]: http://geopandas.org
[OGR]: https://gdal.org/drivers/vector/index.html
[Shapely]: https://shapely.readthedocs.io/en/latest/manual.html
